{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abff942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text,sequence\n",
    "from keras.utils import np_utils,pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df71d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "\n",
    "dl_data=data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a7b9d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75\n",
      "Some words and unique identifire: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Buliding Vocabulary\n",
    "tokenizer=text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id=tokenizer.word_index\n",
    "\n",
    "word2id['PAD']=0\n",
    "id2word={v:k for k,v in word2id.items() }\n",
    "wids=[[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size=len(word2id)\n",
    "embed_size=100\n",
    "window_size=2\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Some words and unique identifire:\", list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47f8c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating context word pairs\n",
    "\n",
    "def generating_context_word_pairs(corpus,window_size,vocab_size):\n",
    "  context_length=window_size*2\n",
    "  for words in corpus:\n",
    "    sentence_length=len(words)\n",
    "    for index,word in enumerate(words):\n",
    "      context_words=[]\n",
    "      label_word=[]\n",
    "      start=index-window_size\n",
    "      end=index+window_size+1\n",
    "      context_words.append([words[i] for i in range(start,end) if 0 <= i< sentence_length and i!=index])\n",
    "      label_word.append(word)\n",
    "\n",
    "      x=pad_sequences(context_words,maxlen=context_length)\n",
    "      y=np_utils.to_categorical(label_word,vocab_size)\n",
    "      yield(x,y)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73b9c47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda_10 (Lambda)          (None, 100)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,075\n",
      "Trainable params: 15,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model buliding\n",
    "import keras.backend as K\n",
    "model=keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=vocab_size,output_dim=embed_size,input_length=window_size*2),\n",
    "    keras.layers.Lambda(lambda x:K.mean(x,axis=1),output_shape=embed_size),\n",
    "    keras.layers.Dense(vocab_size,activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91d0d269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \tLoss: 433.69831895828247\n",
      "Epoch:  2 \tLoss: 428.9176354408264\n",
      "Epoch:  3 \tLoss: 425.36208391189575\n",
      "Epoch:  4 \tLoss: 422.0327968597412\n",
      "Epoch:  5 \tLoss: 419.6004343032837\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "for epoch in range(1,6):\n",
    "  loss=0\n",
    "  i=0\n",
    "  for x,y in generating_context_word_pairs(corpus=wids,window_size=window_size,vocab_size=vocab_size):\n",
    "    loss+=model.train_on_batch(x,y)\n",
    "    i+=1\n",
    "\n",
    "  print(\"Epoch: \",epoch,\"\\tLoss:\",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc5f768a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=model.get_weights()[0]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7fbaaa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>-0.008020</td>\n",
       "      <td>0.045380</td>\n",
       "      <td>-0.016031</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>-0.029912</td>\n",
       "      <td>-0.046721</td>\n",
       "      <td>0.042185</td>\n",
       "      <td>0.035678</td>\n",
       "      <td>-0.048420</td>\n",
       "      <td>-0.042112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071132</td>\n",
       "      <td>-0.022237</td>\n",
       "      <td>0.062520</td>\n",
       "      <td>-0.031422</td>\n",
       "      <td>-0.042875</td>\n",
       "      <td>0.051797</td>\n",
       "      <td>-0.026774</td>\n",
       "      <td>-0.020674</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>-0.060332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.013481</td>\n",
       "      <td>-0.027462</td>\n",
       "      <td>0.026784</td>\n",
       "      <td>0.064938</td>\n",
       "      <td>-0.026745</td>\n",
       "      <td>-0.005874</td>\n",
       "      <td>-0.030811</td>\n",
       "      <td>0.063556</td>\n",
       "      <td>-0.013238</td>\n",
       "      <td>-0.046902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013907</td>\n",
       "      <td>-0.038321</td>\n",
       "      <td>-0.027823</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>-0.013567</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>-0.060280</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>-0.014835</td>\n",
       "      <td>-0.055082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>-0.026907</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>-0.039048</td>\n",
       "      <td>0.028921</td>\n",
       "      <td>0.022194</td>\n",
       "      <td>-0.025694</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>-0.028814</td>\n",
       "      <td>-0.060204</td>\n",
       "      <td>-0.044080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020891</td>\n",
       "      <td>-0.022191</td>\n",
       "      <td>0.065067</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>-0.018249</td>\n",
       "      <td>0.055904</td>\n",
       "      <td>0.038199</td>\n",
       "      <td>-0.029505</td>\n",
       "      <td>0.018243</td>\n",
       "      <td>0.025018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.044706</td>\n",
       "      <td>0.045275</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>-0.024471</td>\n",
       "      <td>-0.047302</td>\n",
       "      <td>0.018649</td>\n",
       "      <td>-0.027069</td>\n",
       "      <td>0.029778</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.049445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012275</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-0.003788</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>-0.032164</td>\n",
       "      <td>-0.022637</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>-0.016424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.039610</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>0.024884</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>-0.034156</td>\n",
       "      <td>-0.036690</td>\n",
       "      <td>-0.049969</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>-0.004311</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020094</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.043448</td>\n",
       "      <td>-0.023720</td>\n",
       "      <td>-0.016987</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>-0.032734</td>\n",
       "      <td>-0.036320</td>\n",
       "      <td>-0.017206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "learning -0.008020  0.045380 -0.016031  0.031390 -0.029912 -0.046721   \n",
       "deep     -0.013481 -0.027462  0.026784  0.064938 -0.026745 -0.005874   \n",
       "networks -0.026907  0.021181 -0.039048  0.028921  0.022194 -0.025694   \n",
       "neural    0.044706  0.045275  0.004538 -0.024471 -0.047302  0.018649   \n",
       "and      -0.039610  0.016444  0.024884  0.046667 -0.034156 -0.036690   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "learning  0.042185  0.035678 -0.048420 -0.042112  ... -0.071132 -0.022237   \n",
       "deep     -0.030811  0.063556 -0.013238 -0.046902  ... -0.013907 -0.038321   \n",
       "networks  0.002333 -0.028814 -0.060204 -0.044080  ...  0.020891 -0.022191   \n",
       "neural   -0.027069  0.029778  0.028577  0.049445  ... -0.012275  0.015505   \n",
       "and      -0.049969  0.019135 -0.004311  0.018540  ... -0.020094 -0.023815   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "learning  0.062520 -0.031422 -0.042875  0.051797 -0.026774 -0.020674   \n",
       "deep     -0.027823  0.028284 -0.013567  0.033972 -0.060280  0.013138   \n",
       "networks  0.065067  0.034070 -0.018249  0.055904  0.038199 -0.029505   \n",
       "neural   -0.003788 -0.010143 -0.045596 -0.032164 -0.022637  0.005350   \n",
       "and       0.009976  0.043448 -0.023720 -0.016987  0.030974 -0.032734   \n",
       "\n",
       "                98        99  \n",
       "learning  0.011435 -0.060332  \n",
       "deep     -0.014835 -0.055082  \n",
       "networks  0.018243  0.025018  \n",
       "neural    0.007236 -0.016424  \n",
       "and      -0.036320 -0.017206  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights, index=list(id2word.values())).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24611a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 75)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distance_matrix=euclidean_distances(weights)\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ba09f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': ['inspection', 'to', 'part', 'some', 'representation'],\n",
       " 'learning': ['deep', 'networks', 'supervised', 'game', 'in']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "for search_term in ['deep','learning']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba6ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a536c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
