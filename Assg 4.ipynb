{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc3b0c1",
   "metadata": {},
   "source": [
    "### 4. Use Autoencoder to implement anomaly detection. Build the model by using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73186517",
   "metadata": {},
   "source": [
    "#### a. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62e400a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1482217a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.112522</td>\n",
       "      <td>-2.827204</td>\n",
       "      <td>-3.773897</td>\n",
       "      <td>-4.349751</td>\n",
       "      <td>-4.376041</td>\n",
       "      <td>-3.474986</td>\n",
       "      <td>-2.181408</td>\n",
       "      <td>-1.818286</td>\n",
       "      <td>-1.250522</td>\n",
       "      <td>-0.477492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792168</td>\n",
       "      <td>0.933541</td>\n",
       "      <td>0.796958</td>\n",
       "      <td>0.578621</td>\n",
       "      <td>0.257740</td>\n",
       "      <td>0.228077</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.925286</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.100878</td>\n",
       "      <td>-3.996840</td>\n",
       "      <td>-4.285843</td>\n",
       "      <td>-4.506579</td>\n",
       "      <td>-4.022377</td>\n",
       "      <td>-3.234368</td>\n",
       "      <td>-1.566126</td>\n",
       "      <td>-0.992258</td>\n",
       "      <td>-0.754680</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538356</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>0.787490</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.773820</td>\n",
       "      <td>1.119621</td>\n",
       "      <td>-1.436250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-2.593450</td>\n",
       "      <td>-3.874230</td>\n",
       "      <td>-4.584095</td>\n",
       "      <td>-4.187449</td>\n",
       "      <td>-3.151462</td>\n",
       "      <td>-1.742940</td>\n",
       "      <td>-1.490659</td>\n",
       "      <td>-1.183580</td>\n",
       "      <td>-0.394229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886073</td>\n",
       "      <td>0.531452</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.021919</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.532197</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-0.421797</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490473</td>\n",
       "      <td>-1.914407</td>\n",
       "      <td>-3.616364</td>\n",
       "      <td>-4.318823</td>\n",
       "      <td>-4.268016</td>\n",
       "      <td>-3.881110</td>\n",
       "      <td>-2.993280</td>\n",
       "      <td>-1.671131</td>\n",
       "      <td>-1.333884</td>\n",
       "      <td>-0.965629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350816</td>\n",
       "      <td>0.499111</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.842069</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>1.086798</td>\n",
       "      <td>1.403011</td>\n",
       "      <td>-0.383564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800232</td>\n",
       "      <td>-0.874252</td>\n",
       "      <td>-2.384761</td>\n",
       "      <td>-3.973292</td>\n",
       "      <td>-4.338224</td>\n",
       "      <td>-3.802422</td>\n",
       "      <td>-2.534510</td>\n",
       "      <td>-1.783423</td>\n",
       "      <td>-1.594450</td>\n",
       "      <td>-0.753199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148884</td>\n",
       "      <td>0.958434</td>\n",
       "      <td>1.059025</td>\n",
       "      <td>1.371682</td>\n",
       "      <td>1.277392</td>\n",
       "      <td>0.960304</td>\n",
       "      <td>0.971020</td>\n",
       "      <td>1.614392</td>\n",
       "      <td>1.421456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>0.608558</td>\n",
       "      <td>-0.335651</td>\n",
       "      <td>-0.990948</td>\n",
       "      <td>-1.784153</td>\n",
       "      <td>-2.626145</td>\n",
       "      <td>-2.957065</td>\n",
       "      <td>-2.931897</td>\n",
       "      <td>-2.664816</td>\n",
       "      <td>-2.090137</td>\n",
       "      <td>-1.461841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.757705</td>\n",
       "      <td>2.291923</td>\n",
       "      <td>2.704595</td>\n",
       "      <td>2.451519</td>\n",
       "      <td>2.017396</td>\n",
       "      <td>1.704358</td>\n",
       "      <td>1.688542</td>\n",
       "      <td>1.629593</td>\n",
       "      <td>1.342651</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>-2.060402</td>\n",
       "      <td>-2.860116</td>\n",
       "      <td>-3.405074</td>\n",
       "      <td>-3.748719</td>\n",
       "      <td>-3.513561</td>\n",
       "      <td>-3.006545</td>\n",
       "      <td>-2.234850</td>\n",
       "      <td>-1.593270</td>\n",
       "      <td>-1.075279</td>\n",
       "      <td>-0.976047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.388947</td>\n",
       "      <td>2.079675</td>\n",
       "      <td>2.433375</td>\n",
       "      <td>2.159484</td>\n",
       "      <td>1.819747</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>1.696818</td>\n",
       "      <td>1.483832</td>\n",
       "      <td>1.047612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-1.122969</td>\n",
       "      <td>-2.252925</td>\n",
       "      <td>-2.867628</td>\n",
       "      <td>-3.358605</td>\n",
       "      <td>-3.167849</td>\n",
       "      <td>-2.638360</td>\n",
       "      <td>-1.664162</td>\n",
       "      <td>-0.935655</td>\n",
       "      <td>-0.866953</td>\n",
       "      <td>-0.645363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472419</td>\n",
       "      <td>-1.310147</td>\n",
       "      <td>-2.029521</td>\n",
       "      <td>-3.221294</td>\n",
       "      <td>-4.176790</td>\n",
       "      <td>-4.009720</td>\n",
       "      <td>-2.874136</td>\n",
       "      <td>-2.008369</td>\n",
       "      <td>-1.808334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.547705</td>\n",
       "      <td>-1.889545</td>\n",
       "      <td>-2.839779</td>\n",
       "      <td>-3.457912</td>\n",
       "      <td>-3.929149</td>\n",
       "      <td>-3.966026</td>\n",
       "      <td>-3.492560</td>\n",
       "      <td>-2.695270</td>\n",
       "      <td>-1.849691</td>\n",
       "      <td>-1.374321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258419</td>\n",
       "      <td>1.907530</td>\n",
       "      <td>2.280888</td>\n",
       "      <td>1.895242</td>\n",
       "      <td>1.437702</td>\n",
       "      <td>1.193433</td>\n",
       "      <td>1.261335</td>\n",
       "      <td>1.150449</td>\n",
       "      <td>0.804932</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-1.351779</td>\n",
       "      <td>-2.209006</td>\n",
       "      <td>-2.520225</td>\n",
       "      <td>-3.061475</td>\n",
       "      <td>-3.065141</td>\n",
       "      <td>-3.030739</td>\n",
       "      <td>-2.622720</td>\n",
       "      <td>-2.044092</td>\n",
       "      <td>-1.295874</td>\n",
       "      <td>-0.733839</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.512234</td>\n",
       "      <td>-2.076075</td>\n",
       "      <td>-2.586042</td>\n",
       "      <td>-3.322799</td>\n",
       "      <td>-3.627311</td>\n",
       "      <td>-3.437038</td>\n",
       "      <td>-2.260023</td>\n",
       "      <td>-1.577823</td>\n",
       "      <td>-0.684531</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4998 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
       "1    -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
       "2    -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
       "3     0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
       "4     0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4993  0.608558 -0.335651 -0.990948 -1.784153 -2.626145 -2.957065 -2.931897   \n",
       "4994 -2.060402 -2.860116 -3.405074 -3.748719 -3.513561 -3.006545 -2.234850   \n",
       "4995 -1.122969 -2.252925 -2.867628 -3.358605 -3.167849 -2.638360 -1.664162   \n",
       "4996 -0.547705 -1.889545 -2.839779 -3.457912 -3.929149 -3.966026 -3.492560   \n",
       "4997 -1.351779 -2.209006 -2.520225 -3.061475 -3.065141 -3.030739 -2.622720   \n",
       "\n",
       "           7         8         9    ...       131       132       133  \\\n",
       "0    -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958   \n",
       "1    -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490   \n",
       "2    -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377   \n",
       "3    -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345   \n",
       "4    -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4993 -2.664816 -2.090137 -1.461841  ...  1.757705  2.291923  2.704595   \n",
       "4994 -1.593270 -1.075279 -0.976047  ...  1.388947  2.079675  2.433375   \n",
       "4995 -0.935655 -0.866953 -0.645363  ... -0.472419 -1.310147 -2.029521   \n",
       "4996 -2.695270 -1.849691 -1.374321  ...  1.258419  1.907530  2.280888   \n",
       "4997 -2.044092 -1.295874 -0.733839  ... -1.512234 -2.076075 -2.586042   \n",
       "\n",
       "           134       135       136       137       138       139  140  \n",
       "0     0.578621  0.257740  0.228077  0.123431  0.925286  0.193137  1.0  \n",
       "1     0.724046  0.555784  0.476333  0.773820  1.119621 -1.436250  1.0  \n",
       "2    -0.021919 -0.713683 -0.532197  0.321097  0.904227 -0.421797  1.0  \n",
       "3     0.842069  0.952074  0.990133  1.086798  1.403011 -0.383564  1.0  \n",
       "4     1.371682  1.277392  0.960304  0.971020  1.614392  1.421456  1.0  \n",
       "...        ...       ...       ...       ...       ...       ...  ...  \n",
       "4993  2.451519  2.017396  1.704358  1.688542  1.629593  1.342651  0.0  \n",
       "4994  2.159484  1.819747  1.534767  1.696818  1.483832  1.047612  0.0  \n",
       "4995 -3.221294 -4.176790 -4.009720 -2.874136 -2.008369 -1.808334  0.0  \n",
       "4996  1.895242  1.437702  1.193433  1.261335  1.150449  0.804932  0.0  \n",
       "4997 -3.322799 -3.627311 -3.437038 -2.260023 -1.577823 -0.684531  0.0  \n",
       "\n",
       "[4998 rows x 141 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv\", header = None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158900da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 141)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f03badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(140, axis=1)\n",
    "target = data[140]\n",
    "x_train, x_test, y_train, y_test = train_test_split( features, target, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "826dc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(keras.Model):\n",
    "    def __init__(self, output_units, ldim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(16, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(ldim, activation='relu')\n",
    "        ])\n",
    "        self.decoder = keras.Sequential([\n",
    "            keras.layers.Dense(16, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Dense(output_units, activation='relu')\n",
    "        ])\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a814f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(output_units=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4be26cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss = 'msle', metrics = ['mse'], optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d61c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 4s 5ms/step - loss: 0.0552 - mse: 0.8722\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.0325 - mse: 0.7979\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0278 - mse: 0.7837\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0253 - mse: 0.7754\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0230 - mse: 0.7677\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.0206 - mse: 0.7592\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0193 - mse: 0.7544\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.0173 - mse: 0.7493\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0165 - mse: 0.7477\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.0159 - mse: 0.7452\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(x_train, x_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55268d6a",
   "metadata": {},
   "source": [
    "#### b. Upload / access the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a04493",
   "metadata": {},
   "source": [
    "#### c. Encoder converts it into latent representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ccd2e",
   "metadata": {},
   "source": [
    "#### d. Decoder networks convert it back to the original input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1804407",
   "metadata": {},
   "source": [
    "#### e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3380550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
